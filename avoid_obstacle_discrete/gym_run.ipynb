{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from gym_unity.envs import UnityToGymWrapper\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定数の設定\n",
    "GAMMA = 0.99 #時間割引率\n",
    "NUM_EPISODES = 1000 #最大試行回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY\n",
    "        self.memory = []\n",
    "        self.index = 0\n",
    "\n",
    "    def push(self, state, action, state_next, reward):\n",
    "\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "\n",
    "        self.memory[self.index] = Transition(state, action, state_next, reward)\n",
    "\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TD_ERROR_EPSILON = 0.0001\n",
    "\n",
    "class TDerrorMemory:\n",
    "\n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY #メモリの最大長\n",
    "        self.memory = []#経験を保存する変数\n",
    "        self.index = 0#保存するindexを示す変数\n",
    "\n",
    "    def push(self, td_error):\n",
    "        '''TD誤差をメモリに保存'''\n",
    "\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "\n",
    "        self.memory[self.index] = td_error\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "\n",
    "    def __len__(self):\n",
    "        '''関数lenに対して、現在の変数memoryの長さを返す'''\n",
    "        return len(self.memory)\n",
    "\n",
    "    def get_prioritized_indexes(self, batch_size):\n",
    "        '''TD誤差に応じた確率でindexを取得'''\n",
    "\n",
    "        #TD誤差の和を計算\n",
    "        sum_absolute_td_error = np.sum(np.absolute(self.memory))\n",
    "        sum_absolute_td_error += TD_ERROR_EPSILON * len(self.memory)\n",
    "\n",
    "        #batch_size分の乱数を生成して、昇順に並べる\n",
    "        rand_list = np.random.uniform(0, sum_absolute_td_error, batch_size)\n",
    "        rand_list = np.sort(rand_list)\n",
    "\n",
    "        #作成した乱数で串刺しにして、インデックスを求める\n",
    "        indexes = []\n",
    "        idx = 0\n",
    "        tmp_sum_absolute_td_error = 0\n",
    "        for rand_num in rand_list:\n",
    "            while tmp_sum_absolute_td_error < rand_num:\n",
    "                tmp_sum_absolute_td_error += (abs(self.memory[idx]) + TD_ERROR_EPSILON)\n",
    "                idx += 1\n",
    "\n",
    "            #微小値を計算に使用した関係でindexがメモリの長さを超えた場合の補正\n",
    "            if idx >= len(self.memory):\n",
    "                idx = len(self.memory) - 1\n",
    "            indexes.append(idx)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def update_td_error(self, updated_td_errors):\n",
    "        '''TD誤差の更新'''\n",
    "        self.memory = updated_td_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, n_mid, n_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_mid)\n",
    "        self.fc2 = nn.Linear(n_mid, n_mid)\n",
    "        self.fc3 = nn.Linear(n_mid, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        output = self.fc3(h2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "CAPACITY = 10000\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions #行動\n",
    "        self.memory = ReplayMemory(CAPACITY)#経験を記憶するメモリオブジェクト\n",
    "\n",
    "        #ニューラルネットワーク構築\n",
    "        n_in, n_mid, n_out = num_states, 32, num_actions\n",
    "        self.main_q_network = Net(n_in, n_mid, n_out)\n",
    "        self.target_q_network = Net(n_in, n_mid, n_out)\n",
    "        print(self.main_q_network)#ネットワークの形を出力\n",
    "\n",
    "        self.optimizer = optim.Adam(self.main_q_network.parameters(), lr=0.0001)#最適化手法の設定\n",
    "\n",
    "        self.td_error_memory = TDerrorMemory(CAPACITY)\n",
    "\n",
    "    def replay(self, episode):\n",
    "        '''Experience Replayでネットワークの結合パラメータを学習'''\n",
    "        #1. メモリサイズの確認\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        #2. ミニバッチ作成\n",
    "        self.batch, self.state_batch, self.action_batch, self.reward_batch, self.non_final_next_states = self.make_minibatch(episode)\n",
    "\n",
    "        #3. 教師信号となるQ(s_t, a_t)値を求める\n",
    "        self.expected_state_action_values = self.get_expected_state_action_values()\n",
    "\n",
    "        #4. 結合パラメータの更新\n",
    "        self.update_main_q_network()\n",
    "\n",
    "    def decide_action(self, state, episode):\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            self.main_q_network.eval()#推論モードに切り替え\n",
    "            with torch.no_grad():\n",
    "                action = self.main_q_network(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            action = torch.LongTensor([[random.randrange(self.num_actions)]])\n",
    "\n",
    "        return action\n",
    "\n",
    "    def make_minibatch(self, episode):\n",
    "        '''2.ミニバッチの作成'''\n",
    "\n",
    "        #2.1 メモリからミニバッチ分のデータを取り出す\n",
    "        if episode < 30:\n",
    "            transitions = self.memory.sample(BATCH_SIZE)\n",
    "        else:\n",
    "            #TD誤差に応じてミニバッチを取り出す\n",
    "            indexes = self.td_error_memory.get_prioritized_indexes(BATCH_SIZE)\n",
    "            transitions = [self.memory.memory[n] for n in indexes]\n",
    "\n",
    "        #2.2 各変数をミニバッチに対応する形に変形\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        #2.3 各変数の要素をミニバッチに対応する形に変形する\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "        return batch, state_batch, action_batch, reward_batch, non_final_next_states\n",
    "\n",
    "    def get_expected_state_action_values(self):\n",
    "        '''3.教師信号となるQ(s_t, a_t)値を求める'''\n",
    "\n",
    "        #3.1 ネットワークを推論モードに切り替え\n",
    "        self.main_q_network.eval()\n",
    "        self.target_q_network.eval()\n",
    "\n",
    "        #3.2 ネットワークが出力したQ(s_t, a_t)を求める\n",
    "        self.state_action_values = self.main_q_network(self.state_batch).gather(1, self.action_batch)\n",
    "\n",
    "        #3.3 max{Q(s_t+1, a)}値を求める。ただし次の状態があるかに注意\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None, self.batch.next_state)))\n",
    "\n",
    "        next_state_values = torch.zeros(BATCH_SIZE)\n",
    "\n",
    "        a_m = torch.zeros(BATCH_SIZE).type(torch.LongTensor)\n",
    "\n",
    "        a_m[non_final_mask] = self.main_q_network(\n",
    "            self.non_final_next_states).detach().max(1)[1]\n",
    "\n",
    "        a_m_non_final_next_states = a_m[non_final_mask].view(-1, 1)\n",
    "\n",
    "        next_state_values[non_final_mask] = self.target_q_network(self.non_final_next_states).gather(1, a_m_non_final_next_states).detach().squeeze()\n",
    "\n",
    "        #3.4 教師となるQ(s_t, a_t)値を、Q学習の式から求める\n",
    "        expected_state_action_values = self.reward_batch + GAMMA * next_state_values\n",
    "\n",
    "        return expected_state_action_values\n",
    "\n",
    "    def update_main_q_network(self):\n",
    "        '''4. 結合パラメータの更新'''\n",
    "\n",
    "        #4.1 ネットワークを訓練モードに切り替える\n",
    "        self.main_q_network.train()\n",
    "\n",
    "        #4.2 損失関数を計算する(smooth_l1_lossはHuberloss)\n",
    "        loss = F.smooth_l1_loss(self.state_action_values, self.expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        #4.3 結合パラメータを更新する\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_q_network(self):\n",
    "        '''Target Q-NetworkをMainと同じにする'''\n",
    "        self.target_q_network.load_state_dict(self.main_q_network.state_dict())\n",
    "\n",
    "    def update_td_error_memory(self):\n",
    "        '''TD誤差メモリに格納されているTD誤差を更新する'''\n",
    "\n",
    "        #ネットワークを推論モードに\n",
    "        self.main_q_network.eval()\n",
    "        self.target_q_network.eval()\n",
    "\n",
    "        #全メモリでミニバッチを作成\n",
    "        transitions = self.memory.memory\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "        #ネットワークが出力したQ(s_t, a_t)を求める\n",
    "        state_action_values = self.main_q_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "        #cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None, batch.next_state)))\n",
    "\n",
    "        #まずは全部0にしておく、サイズはメモリの長さ\n",
    "        next_state_values = torch.zeros(len(self.memory))\n",
    "        a_m = torch.zeros(len(self.memory)).type(torch.LongTensor)\n",
    "\n",
    "        #次の状態での最大Q値の行動a_mをMain Q-networkから求める\n",
    "        a_m[non_final_mask] = self.main_q_network(non_final_next_states).detach().max(1)[1]\n",
    "\n",
    "        #次の状態があるものだけにフィルターし、size 32を32*1へ\n",
    "        a_m_non_final_next_states = a_m[non_final_mask].view(-1, 1)\n",
    "\n",
    "        #次の状態があるindexの、行動a_mのQ値をtarget Q-networkから求める\n",
    "        next_state_values[non_final_mask] = self.target_q_network(non_final_next_states).gather(1, a_m_non_final_next_states).detach().squeeze()\n",
    "\n",
    "        #TD誤差を求める\n",
    "        td_errors = (reward_batch + GAMMA * next_state_values) - \\\n",
    "            state_action_values.squeeze()\n",
    "\n",
    "        #TD誤差メモリを更新、Tensorをdetach()で取り出し、NumPyにしてから、Pythonのリストまで変換\n",
    "        self.td_error_memory.memory = td_errors.detach().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        '''課題の状態と行動の数を設定する'''\n",
    "        self.brain = Brain(num_states, num_actions) \n",
    "\n",
    "    def update_q_function(self, episode):\n",
    "        '''Q関数を更新する'''\n",
    "        self.brain.replay(episode)\n",
    "\n",
    "    def get_action(self, state, episode):\n",
    "        '''行動を決定する'''\n",
    "        action = self.brain.decide_action(state, episode)\n",
    "        return action\n",
    "\n",
    "    def memorize(self, state, action, state_next, reward):\n",
    "        '''memoryオブジェクトに、state, action, state_next, rewardの内容を保存する'''\n",
    "        self.brain.memory.push(state, action, state_next, reward)\n",
    "\n",
    "    def update_target_q_function(self):\n",
    "        '''Target Q-NetworkをMain Q-Networkと同じに更新'''\n",
    "        self.brain.update_target_q_network()\n",
    "        \n",
    "    def memorize_td_error(self, td_error):  \n",
    "        '''TD誤差メモリにTD誤差を格納'''\n",
    "        self.brain.td_error_memory.push(td_error)\n",
    "        \n",
    "    def update_td_error_memory(self): \n",
    "        '''TD誤差メモリに格納されているTD誤差を更新する'''\n",
    "        self.brain.update_td_error_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_directory):\n",
    "    unity_env = UnityEnvironment(env_directory)\n",
    "    env = UnityToGymWrapper(unity_env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.env = make_env(\"ML-agents-test\")\n",
    "        num_states = self.env.observation_space.shape[0]\n",
    "        num_actions = self.env.action_space.n\n",
    "\n",
    "        self.agent = Agent(num_states, num_actions)\n",
    "\n",
    "    def run(self):\n",
    "        episode_10_list = np.zeros(10)\n",
    "\n",
    "        complete_episodes = 0\n",
    "        episode_final = False\n",
    "        frames = []\n",
    "\n",
    "        for episode in range(NUM_EPISODES):\n",
    "            observation = self.env.reset()\n",
    "\n",
    "            state = observation\n",
    "            state = torch.from_numpy(state).type(torch.FloatTensor)\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "\n",
    "            while(1):\n",
    "                \n",
    "                action = self.agent.get_action(state, episode)\n",
    "\n",
    "                observation_next, reward, done, _ = self.env.step(action.item())\n",
    "                reward = torch.FloatTensor([reward])\n",
    "                if done:\n",
    "                    state_next = None\n",
    "                    print(\"done :\" + str(episode))\n",
    "                    break\n",
    "                else:\n",
    "                    state_next = observation_next\n",
    "                    state_next = torch.from_numpy(state_next).type(torch.FloatTensor)\n",
    "                    state_next = torch.unsqueeze(state_next, 0)\n",
    "\n",
    "                self.agent.memorize(state, action, state_next, reward)\n",
    "\n",
    "                self.agent.memorize_td_error(0)\n",
    "\n",
    "                self.agent.update_q_function(episode)\n",
    "\n",
    "                state = state_next\n",
    "\n",
    "                if done:\n",
    "                    self.agent.update_td_error_memory()\n",
    "\n",
    "                    if(episode % 2 == 0):\n",
    "                        self.agent.update_target_q_function()\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected to Unity environment with package version 2.2.1-exp.1 and communication version 1.5.0\n",
      "[INFO] Connected new brain: avoid_obstacle?team=0\n",
      "[WARNING] The environment contains multiple observations. You must define allow_multiple_obs=True to receive them all. Otherwise, only the first visual observation (or vector observation ifthere are no visual observations) will be provided in the observation.\n",
      "Net(\n",
      "  (fc1): Linear(in_features=6, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\anaconda3\\envs\\ML-agents\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done :0\n",
      "done :1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\AppData\\Local\\Temp/ipykernel_19904/1283971446.py:90: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:30.)\n",
      "  a_m[non_final_mask] = self.main_q_network(\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp/ipykernel_19904/1283971446.py:93: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:30.)\n",
      "  a_m_non_final_next_states = a_m[non_final_mask].view(-1, 1)\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp/ipykernel_19904/1283971446.py:95: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:30.)\n",
      "  next_state_values[non_final_mask] = self.target_q_network(self.non_final_next_states).gather(1, a_m_non_final_next_states).detach().squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done :2\n",
      "done :3\n",
      "done :4\n",
      "done :5\n",
      "done :6\n",
      "done :7\n",
      "done :8\n",
      "done :9\n",
      "done :10\n",
      "done :11\n",
      "done :12\n",
      "done :13\n",
      "done :14\n",
      "done :15\n",
      "done :16\n",
      "done :17\n",
      "done :18\n",
      "done :19\n",
      "done :20\n",
      "done :21\n",
      "done :22\n",
      "done :23\n",
      "done :24\n",
      "done :25\n",
      "done :26\n",
      "done :27\n",
      "done :28\n",
      "done :29\n",
      "done :30\n",
      "done :31\n",
      "done :32\n",
      "done :33\n",
      "done :34\n",
      "done :35\n",
      "done :36\n",
      "done :37\n",
      "done :38\n",
      "done :39\n",
      "done :40\n",
      "done :41\n",
      "done :42\n",
      "done :43\n",
      "done :44\n",
      "done :45\n",
      "done :46\n",
      "done :47\n",
      "done :48\n",
      "done :49\n",
      "done :50\n",
      "done :51\n",
      "done :52\n",
      "done :53\n",
      "done :54\n",
      "done :55\n",
      "done :56\n",
      "done :57\n",
      "done :58\n",
      "done :59\n",
      "done :60\n",
      "done :61\n",
      "done :62\n",
      "done :63\n",
      "done :64\n",
      "done :65\n",
      "done :66\n",
      "done :67\n",
      "done :68\n",
      "done :69\n",
      "done :70\n",
      "done :71\n",
      "done :72\n",
      "done :73\n",
      "done :74\n",
      "done :75\n",
      "done :76\n",
      "done :77\n",
      "done :78\n",
      "done :79\n",
      "done :80\n",
      "done :81\n",
      "done :82\n",
      "done :83\n",
      "done :84\n",
      "done :85\n",
      "done :86\n",
      "done :87\n",
      "done :88\n",
      "done :89\n",
      "done :90\n",
      "done :91\n",
      "done :92\n",
      "done :93\n",
      "done :94\n",
      "done :95\n",
      "done :96\n",
      "done :97\n",
      "done :98\n",
      "done :99\n",
      "done :100\n",
      "done :101\n",
      "done :102\n",
      "done :103\n",
      "done :104\n",
      "done :105\n",
      "done :106\n",
      "done :107\n",
      "done :108\n",
      "done :109\n",
      "done :110\n",
      "done :111\n",
      "done :112\n",
      "done :113\n",
      "done :114\n",
      "done :115\n",
      "done :116\n",
      "done :117\n",
      "done :118\n",
      "done :119\n",
      "done :120\n",
      "done :121\n",
      "done :122\n",
      "done :123\n",
      "done :124\n",
      "done :125\n",
      "done :126\n",
      "done :127\n",
      "done :128\n",
      "done :129\n",
      "done :130\n",
      "done :131\n",
      "done :132\n",
      "done :133\n",
      "done :134\n",
      "done :135\n",
      "done :136\n",
      "done :137\n",
      "done :138\n",
      "done :139\n",
      "done :140\n",
      "done :141\n",
      "done :142\n",
      "done :143\n",
      "done :144\n",
      "done :145\n",
      "done :146\n",
      "done :147\n",
      "done :148\n",
      "done :149\n",
      "done :150\n",
      "done :151\n",
      "done :152\n",
      "done :153\n",
      "done :154\n",
      "done :155\n",
      "done :156\n",
      "done :157\n",
      "done :158\n",
      "done :159\n",
      "done :160\n",
      "done :161\n",
      "done :162\n",
      "done :163\n",
      "done :164\n",
      "done :165\n",
      "done :166\n",
      "done :167\n",
      "done :168\n",
      "done :169\n",
      "done :170\n",
      "done :171\n",
      "done :172\n",
      "done :173\n",
      "done :174\n",
      "done :175\n",
      "done :176\n",
      "done :177\n",
      "done :178\n",
      "done :179\n",
      "done :180\n",
      "done :181\n",
      "done :182\n",
      "done :183\n",
      "done :184\n",
      "done :185\n",
      "done :186\n",
      "done :187\n",
      "done :188\n",
      "done :189\n",
      "done :190\n",
      "done :191\n",
      "done :192\n",
      "done :193\n",
      "done :194\n",
      "done :195\n",
      "done :196\n",
      "done :197\n",
      "done :198\n",
      "done :199\n",
      "done :200\n",
      "done :201\n",
      "done :202\n",
      "done :203\n",
      "done :204\n",
      "done :205\n",
      "done :206\n",
      "done :207\n",
      "done :208\n",
      "done :209\n",
      "done :210\n",
      "done :211\n",
      "done :212\n",
      "done :213\n",
      "done :214\n",
      "done :215\n",
      "done :216\n",
      "done :217\n",
      "done :218\n",
      "done :219\n",
      "done :220\n",
      "done :221\n",
      "done :222\n",
      "done :223\n",
      "done :224\n",
      "done :225\n",
      "done :226\n",
      "done :227\n",
      "done :228\n",
      "done :229\n",
      "done :230\n",
      "done :231\n",
      "done :232\n",
      "done :233\n",
      "done :234\n",
      "done :235\n",
      "done :236\n",
      "done :237\n",
      "done :238\n",
      "done :239\n",
      "done :240\n",
      "done :241\n",
      "done :242\n",
      "done :243\n",
      "done :244\n",
      "done :245\n",
      "done :246\n",
      "done :247\n",
      "done :248\n",
      "done :249\n",
      "done :250\n",
      "done :251\n",
      "done :252\n",
      "done :253\n",
      "done :254\n",
      "done :255\n",
      "done :256\n",
      "done :257\n",
      "done :258\n",
      "done :259\n",
      "done :260\n",
      "done :261\n",
      "done :262\n",
      "done :263\n",
      "done :264\n",
      "done :265\n",
      "done :266\n",
      "done :267\n",
      "done :268\n",
      "done :269\n",
      "done :270\n",
      "done :271\n",
      "done :272\n",
      "done :273\n",
      "done :274\n",
      "done :275\n",
      "done :276\n",
      "done :277\n",
      "done :278\n",
      "done :279\n",
      "done :280\n",
      "done :281\n",
      "done :282\n",
      "done :283\n",
      "done :284\n",
      "done :285\n",
      "done :286\n",
      "done :287\n",
      "done :288\n",
      "done :289\n",
      "done :290\n",
      "done :291\n",
      "done :292\n",
      "done :293\n",
      "done :294\n",
      "done :295\n",
      "done :296\n",
      "done :297\n",
      "done :298\n",
      "done :299\n",
      "done :300\n",
      "done :301\n",
      "done :302\n",
      "done :303\n",
      "done :304\n",
      "done :305\n",
      "done :306\n",
      "done :307\n",
      "done :308\n",
      "done :309\n",
      "done :310\n",
      "done :311\n",
      "done :312\n",
      "done :313\n",
      "done :314\n",
      "done :315\n",
      "done :316\n",
      "done :317\n",
      "done :318\n",
      "done :319\n",
      "done :320\n",
      "done :321\n",
      "done :322\n",
      "done :323\n",
      "done :324\n",
      "done :325\n",
      "done :326\n",
      "done :327\n",
      "done :328\n",
      "done :329\n",
      "done :330\n",
      "done :331\n",
      "done :332\n",
      "done :333\n",
      "done :334\n",
      "done :335\n",
      "done :336\n",
      "done :337\n",
      "done :338\n",
      "done :339\n",
      "done :340\n",
      "done :341\n",
      "done :342\n",
      "done :343\n",
      "done :344\n",
      "done :345\n",
      "done :346\n",
      "done :347\n",
      "done :348\n",
      "done :349\n",
      "done :350\n",
      "done :351\n",
      "done :352\n",
      "done :353\n",
      "done :354\n",
      "done :355\n",
      "done :356\n",
      "done :357\n",
      "done :358\n",
      "done :359\n",
      "done :360\n",
      "done :361\n",
      "done :362\n",
      "done :363\n",
      "done :364\n",
      "done :365\n",
      "done :366\n",
      "done :367\n",
      "done :368\n",
      "done :369\n",
      "done :370\n",
      "done :371\n",
      "done :372\n",
      "done :373\n",
      "done :374\n",
      "done :375\n",
      "done :376\n",
      "done :377\n",
      "done :378\n",
      "done :379\n",
      "done :380\n",
      "done :381\n",
      "done :382\n",
      "done :383\n",
      "done :384\n",
      "done :385\n",
      "done :386\n",
      "done :387\n",
      "done :388\n",
      "done :389\n",
      "done :390\n",
      "done :391\n",
      "done :392\n",
      "done :393\n",
      "done :394\n",
      "done :395\n",
      "done :396\n",
      "done :397\n",
      "done :398\n",
      "done :399\n",
      "done :400\n",
      "done :401\n",
      "done :402\n",
      "done :403\n",
      "done :404\n",
      "done :405\n",
      "done :406\n",
      "done :407\n",
      "done :408\n",
      "done :409\n",
      "done :410\n",
      "done :411\n",
      "done :412\n",
      "done :413\n",
      "done :414\n",
      "done :415\n",
      "done :416\n",
      "done :417\n",
      "done :418\n",
      "done :419\n",
      "done :420\n",
      "done :421\n",
      "done :422\n",
      "done :423\n",
      "done :424\n",
      "done :425\n",
      "done :426\n",
      "done :427\n",
      "done :428\n",
      "done :429\n",
      "done :430\n",
      "done :431\n",
      "done :432\n",
      "done :433\n",
      "done :434\n",
      "done :435\n",
      "done :436\n",
      "done :437\n",
      "done :438\n",
      "done :439\n",
      "done :440\n",
      "done :441\n",
      "done :442\n",
      "done :443\n",
      "done :444\n",
      "done :445\n",
      "done :446\n",
      "done :447\n",
      "done :448\n",
      "done :449\n",
      "done :450\n",
      "done :451\n",
      "done :452\n",
      "done :453\n",
      "done :454\n",
      "done :455\n",
      "done :456\n",
      "done :457\n",
      "done :458\n",
      "done :459\n",
      "done :460\n",
      "done :461\n",
      "done :462\n",
      "done :463\n",
      "done :464\n",
      "done :465\n",
      "done :466\n",
      "done :467\n",
      "done :468\n",
      "done :469\n",
      "done :470\n",
      "done :471\n",
      "done :472\n",
      "done :473\n",
      "done :474\n",
      "done :475\n",
      "done :476\n",
      "done :477\n",
      "done :478\n",
      "done :479\n",
      "done :480\n",
      "done :481\n",
      "done :482\n",
      "done :483\n",
      "done :484\n",
      "done :485\n",
      "done :486\n",
      "done :487\n",
      "done :488\n",
      "done :489\n",
      "done :490\n",
      "done :491\n",
      "done :492\n",
      "done :493\n",
      "done :494\n",
      "done :495\n",
      "done :496\n",
      "done :497\n",
      "done :498\n",
      "done :499\n",
      "done :500\n",
      "done :501\n",
      "done :502\n",
      "done :503\n",
      "done :504\n",
      "done :505\n",
      "done :506\n",
      "done :507\n",
      "done :508\n",
      "done :509\n",
      "done :510\n",
      "done :511\n",
      "done :512\n",
      "done :513\n",
      "done :514\n",
      "done :515\n",
      "done :516\n",
      "done :517\n",
      "done :518\n",
      "done :519\n",
      "done :520\n",
      "done :521\n",
      "done :522\n",
      "done :523\n",
      "done :524\n",
      "done :525\n",
      "done :526\n",
      "done :527\n",
      "done :528\n",
      "done :529\n",
      "done :530\n",
      "done :531\n",
      "done :532\n",
      "done :533\n",
      "done :534\n",
      "done :535\n",
      "done :536\n",
      "done :537\n",
      "done :538\n",
      "done :539\n",
      "done :540\n",
      "done :541\n",
      "done :542\n",
      "done :543\n",
      "done :544\n",
      "done :545\n",
      "done :546\n",
      "done :547\n",
      "done :548\n",
      "done :549\n",
      "done :550\n",
      "done :551\n",
      "done :552\n",
      "done :553\n",
      "done :554\n",
      "done :555\n",
      "done :556\n",
      "done :557\n",
      "done :558\n",
      "done :559\n",
      "done :560\n",
      "done :561\n",
      "done :562\n",
      "done :563\n",
      "done :564\n",
      "done :565\n",
      "done :566\n",
      "done :567\n",
      "done :568\n",
      "done :569\n",
      "done :570\n",
      "done :571\n",
      "done :572\n",
      "done :573\n",
      "done :574\n",
      "done :575\n",
      "done :576\n",
      "done :577\n",
      "done :578\n",
      "done :579\n",
      "done :580\n",
      "done :581\n",
      "done :582\n",
      "done :583\n",
      "done :584\n",
      "done :585\n",
      "done :586\n",
      "done :587\n",
      "done :588\n",
      "done :589\n",
      "done :590\n",
      "done :591\n",
      "done :592\n",
      "done :593\n",
      "done :594\n",
      "done :595\n",
      "done :596\n",
      "done :597\n",
      "done :598\n",
      "done :599\n",
      "done :600\n",
      "done :601\n",
      "done :602\n",
      "done :603\n",
      "done :604\n",
      "done :605\n",
      "done :606\n",
      "done :607\n",
      "done :608\n",
      "done :609\n",
      "done :610\n",
      "done :611\n",
      "done :612\n",
      "done :613\n",
      "done :614\n",
      "done :615\n",
      "done :616\n",
      "done :617\n",
      "done :618\n",
      "done :619\n",
      "done :620\n",
      "done :621\n",
      "done :622\n",
      "done :623\n",
      "done :624\n",
      "done :625\n",
      "done :626\n",
      "done :627\n",
      "done :628\n",
      "done :629\n",
      "done :630\n",
      "done :631\n",
      "done :632\n",
      "done :633\n",
      "done :634\n",
      "done :635\n",
      "done :636\n",
      "done :637\n",
      "done :638\n",
      "done :639\n",
      "done :640\n",
      "done :641\n",
      "done :642\n",
      "done :643\n",
      "done :644\n",
      "done :645\n",
      "done :646\n",
      "done :647\n",
      "done :648\n",
      "done :649\n",
      "done :650\n",
      "done :651\n",
      "done :652\n",
      "done :653\n",
      "done :654\n",
      "done :655\n",
      "done :656\n",
      "done :657\n",
      "done :658\n",
      "done :659\n",
      "done :660\n",
      "done :661\n",
      "done :662\n",
      "done :663\n",
      "done :664\n",
      "done :665\n",
      "done :666\n",
      "done :667\n",
      "done :668\n",
      "done :669\n",
      "done :670\n",
      "done :671\n",
      "done :672\n",
      "done :673\n",
      "done :674\n",
      "done :675\n",
      "done :676\n",
      "done :677\n",
      "done :678\n",
      "done :679\n",
      "done :680\n",
      "done :681\n",
      "done :682\n",
      "done :683\n",
      "done :684\n",
      "done :685\n",
      "done :686\n",
      "done :687\n",
      "done :688\n",
      "done :689\n",
      "done :690\n",
      "done :691\n",
      "done :692\n",
      "done :693\n",
      "done :694\n",
      "done :695\n",
      "done :696\n",
      "done :697\n",
      "done :698\n",
      "done :699\n",
      "done :700\n",
      "done :701\n",
      "done :702\n",
      "done :703\n",
      "done :704\n",
      "done :705\n",
      "done :706\n",
      "done :707\n",
      "done :708\n",
      "done :709\n",
      "done :710\n",
      "done :711\n",
      "done :712\n",
      "done :713\n",
      "done :714\n",
      "done :715\n",
      "done :716\n",
      "done :717\n",
      "done :718\n",
      "done :719\n",
      "done :720\n",
      "done :721\n",
      "done :722\n",
      "done :723\n",
      "done :724\n",
      "done :725\n",
      "done :726\n",
      "done :727\n",
      "done :728\n",
      "done :729\n",
      "done :730\n",
      "done :731\n",
      "done :732\n",
      "done :733\n",
      "done :734\n",
      "done :735\n",
      "done :736\n",
      "done :737\n",
      "done :738\n",
      "done :739\n",
      "done :740\n",
      "done :741\n",
      "done :742\n",
      "done :743\n",
      "done :744\n",
      "done :745\n",
      "done :746\n",
      "done :747\n",
      "done :748\n",
      "done :749\n",
      "done :750\n",
      "done :751\n",
      "done :752\n",
      "done :753\n",
      "done :754\n",
      "done :755\n",
      "done :756\n",
      "done :757\n",
      "done :758\n",
      "done :759\n",
      "done :760\n",
      "done :761\n",
      "done :762\n",
      "done :763\n",
      "done :764\n",
      "done :765\n",
      "done :766\n",
      "done :767\n",
      "done :768\n",
      "done :769\n",
      "done :770\n",
      "done :771\n",
      "done :772\n",
      "done :773\n",
      "done :774\n",
      "done :775\n",
      "done :776\n",
      "done :777\n",
      "done :778\n",
      "done :779\n",
      "done :780\n",
      "done :781\n",
      "done :782\n",
      "done :783\n",
      "done :784\n",
      "done :785\n",
      "done :786\n",
      "done :787\n",
      "done :788\n",
      "done :789\n",
      "done :790\n",
      "done :791\n",
      "done :792\n",
      "done :793\n",
      "done :794\n",
      "done :795\n",
      "done :796\n",
      "done :797\n",
      "done :798\n",
      "done :799\n",
      "done :800\n",
      "done :801\n",
      "done :802\n",
      "done :803\n",
      "done :804\n",
      "done :805\n",
      "done :806\n",
      "done :807\n",
      "done :808\n",
      "done :809\n",
      "done :810\n",
      "done :811\n",
      "done :812\n",
      "done :813\n",
      "done :814\n",
      "done :815\n",
      "done :816\n",
      "done :817\n",
      "done :818\n",
      "done :819\n",
      "done :820\n",
      "done :821\n",
      "done :822\n",
      "done :823\n",
      "done :824\n",
      "done :825\n",
      "done :826\n",
      "done :827\n",
      "done :828\n",
      "done :829\n",
      "done :830\n",
      "done :831\n",
      "done :832\n",
      "done :833\n",
      "done :834\n",
      "done :835\n",
      "done :836\n",
      "done :837\n",
      "done :838\n",
      "done :839\n",
      "done :840\n",
      "done :841\n",
      "done :842\n",
      "done :843\n",
      "done :844\n",
      "done :845\n",
      "done :846\n",
      "done :847\n",
      "done :848\n",
      "done :849\n",
      "done :850\n",
      "done :851\n",
      "done :852\n",
      "done :853\n",
      "done :854\n",
      "done :855\n",
      "done :856\n",
      "done :857\n",
      "done :858\n",
      "done :859\n",
      "done :860\n",
      "done :861\n",
      "done :862\n",
      "done :863\n",
      "done :864\n",
      "done :865\n",
      "done :866\n",
      "done :867\n",
      "done :868\n",
      "done :869\n",
      "done :870\n",
      "done :871\n",
      "done :872\n",
      "done :873\n",
      "done :874\n",
      "done :875\n",
      "done :876\n",
      "done :877\n",
      "done :878\n",
      "done :879\n",
      "done :880\n",
      "done :881\n",
      "done :882\n",
      "done :883\n",
      "done :884\n",
      "done :885\n",
      "done :886\n",
      "done :887\n",
      "done :888\n",
      "done :889\n",
      "done :890\n",
      "done :891\n",
      "done :892\n",
      "done :893\n",
      "done :894\n",
      "done :895\n",
      "done :896\n",
      "done :897\n",
      "done :898\n",
      "done :899\n",
      "done :900\n",
      "done :901\n",
      "done :902\n",
      "done :903\n",
      "done :904\n",
      "done :905\n",
      "done :906\n",
      "done :907\n",
      "done :908\n",
      "done :909\n",
      "done :910\n",
      "done :911\n",
      "done :912\n",
      "done :913\n",
      "done :914\n",
      "done :915\n",
      "done :916\n",
      "done :917\n",
      "done :918\n",
      "done :919\n",
      "done :920\n",
      "done :921\n",
      "done :922\n",
      "done :923\n",
      "done :924\n",
      "done :925\n",
      "done :926\n",
      "done :927\n",
      "done :928\n",
      "done :929\n",
      "done :930\n",
      "done :931\n",
      "done :932\n",
      "done :933\n",
      "done :934\n",
      "done :935\n",
      "done :936\n",
      "done :937\n",
      "done :938\n",
      "done :939\n",
      "done :940\n",
      "done :941\n",
      "done :942\n",
      "done :943\n",
      "done :944\n",
      "done :945\n",
      "done :946\n",
      "done :947\n",
      "done :948\n",
      "done :949\n",
      "done :950\n",
      "done :951\n",
      "done :952\n",
      "done :953\n",
      "done :954\n",
      "done :955\n",
      "done :956\n",
      "done :957\n",
      "done :958\n",
      "done :959\n",
      "done :960\n",
      "done :961\n",
      "done :962\n",
      "done :963\n",
      "done :964\n",
      "done :965\n",
      "done :966\n",
      "done :967\n",
      "done :968\n",
      "done :969\n",
      "done :970\n",
      "done :971\n",
      "done :972\n",
      "done :973\n",
      "done :974\n",
      "done :975\n",
      "done :976\n",
      "done :977\n",
      "done :978\n",
      "done :979\n",
      "done :980\n",
      "done :981\n",
      "done :982\n",
      "done :983\n",
      "done :984\n",
      "done :985\n",
      "done :986\n",
      "done :987\n",
      "done :988\n",
      "done :989\n",
      "done :990\n",
      "done :991\n",
      "done :992\n",
      "done :993\n",
      "done :994\n",
      "done :995\n",
      "done :996\n",
      "done :997\n",
      "done :998\n",
      "done :999\n"
     ]
    }
   ],
   "source": [
    "avoid_obstacle_env = Environment()\n",
    "avoid_obstacle_env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1022e1678fe7ac202b98b6a37d8039da539983b925d150fb4e724e561adf4fa6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ML-agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
