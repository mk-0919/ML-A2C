{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from gym_unity.envs import UnityToGymWrapper\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定数の設定\n",
    "GAMMA = 0.99 #時間割引率\n",
    "NUM_EPISODES = 100 #最大試行回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY\n",
    "        self.memory = []\n",
    "        self.index = 0\n",
    "\n",
    "    def push(self, state, action, state_next, reward):\n",
    "\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "\n",
    "        self.memory[self.index] = Transition(state, action, state_next, reward)\n",
    "\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TD_ERROR_EPSILON = 0.0001\n",
    "\n",
    "class TDerrorMemory:\n",
    "\n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY #メモリの最大長\n",
    "        self.memory = []#経験を保存する変数\n",
    "        self.index = 0#保存するindexを示す変数\n",
    "\n",
    "    def push(self, td_error):\n",
    "        '''TD誤差をメモリに保存'''\n",
    "\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "\n",
    "        self.memory[self.index] = td_error\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "\n",
    "    def __len__(self):\n",
    "        '''関数lenに対して、現在の変数memoryの長さを返す'''\n",
    "        return len(self.memory)\n",
    "\n",
    "    def get_prioritized_indexes(self, batch_size):\n",
    "        '''TD誤差に応じた確率でindexを取得'''\n",
    "\n",
    "        #TD誤差の和を計算\n",
    "        sum_absolute_td_error = np.sum(np.absolute(self.memory))\n",
    "        sum_absolute_td_error += TD_ERROR_EPSILON * len(self.memory)\n",
    "\n",
    "        #batch_size分の乱数を生成して、昇順に並べる\n",
    "        rand_list = np.random.uniform(0, sum_absolute_td_error, batch_size)\n",
    "        rand_list = np.sort(rand_list)\n",
    "\n",
    "        #作成した乱数で串刺しにして、インデックスを求める\n",
    "        indexes = []\n",
    "        idx = 0\n",
    "        tmp_sum_absolute_td_error = 0\n",
    "        for rand_num in rand_list:\n",
    "            while tmp_sum_absolute_td_error < rand_num:\n",
    "                tmp_sum_absolute_td_error += (abs(self.memory[idx]) + TD_ERROR_EPSILON)\n",
    "                idx += 1\n",
    "\n",
    "            #微小値を計算に使用した関係でindexがメモリの長さを超えた場合の補正\n",
    "            if idx >= len(self.memory):\n",
    "                idx = len(self.memory) - 1\n",
    "            indexes.append(idx)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def update_td_error(self, updated_td_errors):\n",
    "        '''TD誤差の更新'''\n",
    "        self.memory = updated_td_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, n_mid, n_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_mid)\n",
    "        self.fc2 = nn.Linear(n_mid, n_mid)\n",
    "        self.fc3 = nn.Linear(n_mid, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        output = self.fc3(h2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "CAPACITY = 10000\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_states = num_states #入力\n",
    "        self.num_actions = num_actions #行動\n",
    "        self.memory = ReplayMemory(CAPACITY)#経験を記憶するメモリオブジェクト\n",
    "\n",
    "        #ニューラルネットワーク構築\n",
    "        n_in, n_mid, n_out = num_states, 32, num_actions\n",
    "        self.main_q_network = Net(n_in, n_mid, n_out)\n",
    "        self.target_q_network = Net(n_in, n_mid, n_out)\n",
    "        print(self.main_q_network)#ネットワークの形を出力\n",
    "\n",
    "        self.optimizer = optim.Adam(self.main_q_network.parameters(), lr=0.0001)#最適化手法の設定\n",
    "\n",
    "        self.td_error_memory = TDerrorMemory(CAPACITY)\n",
    "\n",
    "    def replay(self, episode):\n",
    "        '''Experience Replayでネットワークの結合パラメータを学習'''\n",
    "        #1. メモリサイズの確認\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        #2. ミニバッチ作成\n",
    "        self.batch, self.state_batch, self.action_batch, self.reward_batch, self.non_final_next_states = self.make_minibatch(episode)\n",
    "\n",
    "        #3. 教師信号となるQ(s_t, a_t)値を求める\n",
    "        self.expected_state_action_values = self.get_expected_state_action_values()\n",
    "\n",
    "        #4. 結合パラメータの更新\n",
    "        self.update_main_q_network()\n",
    "\n",
    "    def decide_action(self, state, episode):\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            self.main_q_network.eval()#推論モードに切り替え\n",
    "            with torch.no_grad():\n",
    "                action = self.main_q_network(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            action = torch.LongTensor([[random.randrange(self.num_actions)]])\n",
    "\n",
    "        return action\n",
    "\n",
    "    def make_minibatch(self, episode):\n",
    "        '''2.ミニバッチの作成'''\n",
    "\n",
    "        #2.1 メモリからミニバッチ分のデータを取り出す\n",
    "        if episode < 30:\n",
    "            transitions = self.memory.sample(BATCH_SIZE)\n",
    "        else:\n",
    "            #TD誤差に応じてミニバッチを取り出す\n",
    "            indexes = self.td_error_memory.get_prioritized_indexes(BATCH_SIZE)\n",
    "            transitions = [self.memory.memory[n] for n in indexes]\n",
    "\n",
    "        #2.2 各変数をミニバッチに対応する形に変形\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        #2.3 各変数の要素をミニバッチに対応する形に変形する\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "        return batch, state_batch, action_batch, reward_batch, non_final_next_states\n",
    "\n",
    "    def get_expected_state_action_values(self):\n",
    "        '''3.教師信号となるQ(s_t, a_t)値を求める'''\n",
    "\n",
    "        #3.1 ネットワークを推論モードに切り替え\n",
    "        self.main_q_network.eval()\n",
    "        self.target_q_network.eval()\n",
    "\n",
    "        #3.2 ネットワークが出力したQ(s_t, a_t)を求める\n",
    "        self.state_action_values = self.main_q_network(self.state_batch).gather(1, self.action_batch)\n",
    "\n",
    "        #3.3 max{Q(s_t+1, a)}値を求める。ただし次の状態があるかに注意\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None, self.batch.next_state)))\n",
    "\n",
    "        next_state_values = torch.zeros(BATCH_SIZE)\n",
    "\n",
    "        a_m = torch.zeros(BATCH_SIZE).type(torch.LongTensor)\n",
    "\n",
    "        a_m[non_final_mask] = self.main_q_network(\n",
    "            self.non_final_next_states).detach().max(1)[1]\n",
    "\n",
    "        a_m_non_final_next_states = a_m[non_final_mask].view(-1, 1)\n",
    "\n",
    "        next_state_values[non_final_mask] = self.target_q_network(self.non_final_next_states).gather(1, a_m_non_final_next_states).detach().squeeze()\n",
    "\n",
    "        #3.4 教師となるQ(s_t, a_t)値を、Q学習の式から求める\n",
    "        expected_state_action_values = self.reward_batch + GAMMA * next_state_values\n",
    "\n",
    "        return expected_state_action_values\n",
    "\n",
    "    def update_main_q_network(self):\n",
    "        '''4. 結合パラメータの更新'''\n",
    "\n",
    "        #4.1 ネットワークを訓練モードに切り替える\n",
    "        self.main_q_network.train()\n",
    "\n",
    "        #4.2 損失関数を計算する(smooth_l1_lossはHuberloss)\n",
    "        loss = F.smooth_l1_loss(self.state_action_values, self.expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        #4.3 結合パラメータを更新する\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_q_network(self):\n",
    "        '''Target Q-NetworkをMainと同じにする'''\n",
    "        self.target_q_network.load_state_dict(self.main_q_network.state_dict())\n",
    "\n",
    "    def update_td_error_memory(self):\n",
    "        '''TD誤差メモリに格納されているTD誤差を更新する'''\n",
    "\n",
    "        #ネットワークを推論モードに\n",
    "        self.main_q_network.eval()\n",
    "        self.target_q_network.eval()\n",
    "\n",
    "        #全メモリでミニバッチを作成\n",
    "        transitions = self.memory.memory\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "        #ネットワークが出力したQ(s_t, a_t)を求める\n",
    "        state_action_values = self.main_q_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "        #cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None, batch.next_state)))\n",
    "\n",
    "        #まずは全部0にしておく、サイズはメモリの長さ\n",
    "        next_state_values = torch.zeros(len(self.memory))\n",
    "        a_m = torch.zeros(len(self.memory)).type(torch.LongTensor)\n",
    "\n",
    "        #次の状態での最大Q値の行動a_mをMain Q-networkから求める\n",
    "        a_m[non_final_mask] = self.main_q_network(non_final_next_states).detach().max(1)[1]\n",
    "\n",
    "        #次の状態があるものだけにフィルターし、size 32を32*1へ\n",
    "        a_m_non_final_next_states = a_m[non_final_mask].view(-1, 1)\n",
    "\n",
    "        #次の状態があるindexの、行動a_mのQ値をtarget Q-networkから求める\n",
    "        next_state_values[non_final_mask] = self.target_q_network(non_final_next_states).gather(1, a_m_non_final_next_states).detach().squeeze()\n",
    "\n",
    "        #TD誤差を求める\n",
    "        td_errors = (reward_batch + GAMMA * next_state_values) - \\\n",
    "            state_action_values.squeeze()\n",
    "\n",
    "        #TD誤差メモリを更新、Tensorをdetach()で取り出し、NumPyにしてから、Pythonのリストまで変換\n",
    "        self.td_error_memory.memory = td_errors.detach().numpy().tolist()\n",
    "\n",
    "    def save_onnx(self, output_name):\n",
    "        self.main_q_network.eval()\n",
    "        x = torch.rand(self.num_states)\n",
    "        torch.onnx.export(self.main_q_network, x,\n",
    "            output_name, export_params=True, opset_version=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        '''課題の状態と行動の数を設定する'''\n",
    "        self.brain = Brain(num_states, num_actions) \n",
    "\n",
    "    def update_q_function(self, episode):\n",
    "        '''Q関数を更新する'''\n",
    "        self.brain.replay(episode)\n",
    "\n",
    "    def get_action(self, state, episode):\n",
    "        '''行動を決定する'''\n",
    "        action = self.brain.decide_action(state, episode)\n",
    "        return action\n",
    "\n",
    "    def memorize(self, state, action, state_next, reward):\n",
    "        '''memoryオブジェクトに、state, action, state_next, rewardの内容を保存する'''\n",
    "        self.brain.memory.push(state, action, state_next, reward)\n",
    "\n",
    "    def update_target_q_function(self):\n",
    "        '''Target Q-NetworkをMain Q-Networkと同じに更新'''\n",
    "        self.brain.update_target_q_network()\n",
    "        \n",
    "    def memorize_td_error(self, td_error):  \n",
    "        '''TD誤差メモリにTD誤差を格納'''\n",
    "        self.brain.td_error_memory.push(td_error)\n",
    "        \n",
    "    def update_td_error_memory(self): \n",
    "        '''TD誤差メモリに格納されているTD誤差を更新する'''\n",
    "        self.brain.update_td_error_memory()\n",
    "\n",
    "    def save_onnx(self, output_name):\n",
    "        self.brain.save_onnx(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_directory):\n",
    "    unity_env = UnityEnvironment(env_directory)\n",
    "    env = UnityToGymWrapper(unity_env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.env = make_env(\"ML-agents-test\")\n",
    "        num_states = self.env.observation_space.shape[0]\n",
    "        num_actions = self.env.action_space.n\n",
    "\n",
    "        self.agent = Agent(num_states, num_actions)\n",
    "\n",
    "    def run(self):\n",
    "        episode_10_list = np.zeros(10)\n",
    "\n",
    "        complete_episodes = 0\n",
    "        episode_final = False\n",
    "        frames = []\n",
    "\n",
    "        for episode in range(NUM_EPISODES):\n",
    "            observation = self.env.reset()\n",
    "\n",
    "            state = observation\n",
    "            state = torch.from_numpy(state).type(torch.FloatTensor)\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "\n",
    "            while(1):\n",
    "                \n",
    "                action = self.agent.get_action(state, episode)\n",
    "\n",
    "                observation_next, reward, done, _ = self.env.step(action.item())\n",
    "                reward = torch.FloatTensor([reward])\n",
    "                if done:\n",
    "                    state_next = None\n",
    "                    print(\"done :\" + str(episode))\n",
    "                    break\n",
    "                else:\n",
    "                    state_next = observation_next\n",
    "                    state_next = torch.from_numpy(state_next).type(torch.FloatTensor)\n",
    "                    state_next = torch.unsqueeze(state_next, 0)\n",
    "\n",
    "                self.agent.memorize(state, action, state_next, reward)\n",
    "\n",
    "                self.agent.memorize_td_error(0)\n",
    "\n",
    "                self.agent.update_q_function(episode)\n",
    "\n",
    "                state = state_next\n",
    "\n",
    "                if done:\n",
    "                    self.agent.update_td_error_memory()\n",
    "\n",
    "                    if(episode % 2 == 0):\n",
    "                        self.agent.update_target_q_function()\n",
    "                    break\n",
    "\n",
    "        self.agent.save_onnx(\"test.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected to Unity environment with package version 2.2.1-exp.1 and communication version 1.5.0\n",
      "[INFO] Connected new brain: avoid_obstacle?team=0\n",
      "[WARNING] The environment contains multiple observations. You must define allow_multiple_obs=True to receive them all. Otherwise, only the first visual observation (or vector observation ifthere are no visual observations) will be provided in the observation.\n",
      "Net(\n",
      "  (fc1): Linear(in_features=6, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\anaconda3\\envs\\ML-agents\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done :0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\AppData\\Local\\Temp/ipykernel_38540/898216773.py:91: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:30.)\n",
      "  a_m[non_final_mask] = self.main_q_network(\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp/ipykernel_38540/898216773.py:94: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:30.)\n",
      "  a_m_non_final_next_states = a_m[non_final_mask].view(-1, 1)\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp/ipykernel_38540/898216773.py:96: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:30.)\n",
      "  next_state_values[non_final_mask] = self.target_q_network(self.non_final_next_states).gather(1, a_m_non_final_next_states).detach().squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done :1\n",
      "done :2\n",
      "done :3\n",
      "done :4\n",
      "done :5\n",
      "done :6\n",
      "done :7\n",
      "done :8\n",
      "done :9\n",
      "done :10\n",
      "done :11\n",
      "done :12\n",
      "done :13\n",
      "done :14\n",
      "done :15\n",
      "done :16\n",
      "done :17\n",
      "done :18\n",
      "done :19\n",
      "done :20\n",
      "done :21\n",
      "done :22\n",
      "done :23\n",
      "done :24\n",
      "done :25\n",
      "done :26\n",
      "done :27\n",
      "done :28\n",
      "done :29\n",
      "done :30\n",
      "done :31\n",
      "done :32\n",
      "done :33\n",
      "done :34\n",
      "done :35\n",
      "done :36\n",
      "done :37\n",
      "done :38\n",
      "done :39\n",
      "done :40\n",
      "done :41\n",
      "done :42\n",
      "done :43\n",
      "done :44\n",
      "done :45\n",
      "done :46\n",
      "done :47\n",
      "done :48\n",
      "done :49\n",
      "done :50\n",
      "done :51\n",
      "done :52\n",
      "done :53\n",
      "done :54\n",
      "done :55\n",
      "done :56\n",
      "done :57\n",
      "done :58\n",
      "done :59\n",
      "done :60\n",
      "done :61\n",
      "done :62\n",
      "done :63\n",
      "done :64\n",
      "done :65\n",
      "done :66\n",
      "done :67\n",
      "done :68\n",
      "done :69\n",
      "done :70\n",
      "done :71\n",
      "done :72\n",
      "done :73\n",
      "done :74\n",
      "done :75\n",
      "done :76\n",
      "done :77\n",
      "done :78\n",
      "done :79\n",
      "done :80\n",
      "done :81\n",
      "done :82\n",
      "done :83\n",
      "done :84\n",
      "done :85\n",
      "done :86\n",
      "done :87\n",
      "done :88\n",
      "done :89\n",
      "done :90\n",
      "done :91\n",
      "done :92\n",
      "done :93\n",
      "done :94\n",
      "done :95\n",
      "done :96\n",
      "done :97\n",
      "done :98\n",
      "done :99\n"
     ]
    }
   ],
   "source": [
    "avoid_obstacle_env = Environment()\n",
    "avoid_obstacle_env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3185, -1.8588, -0.8598, -1.3134,  1.0143,  0.5290])\n"
     ]
    }
   ],
   "source": [
    "print(torch.randn(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1022e1678fe7ac202b98b6a37d8039da539983b925d150fb4e724e561adf4fa6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ML-agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
